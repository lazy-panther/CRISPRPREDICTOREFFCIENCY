{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"DT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>WT74_On</th>\n",
       "      <th>Edited74_On</th>\n",
       "      <th>RT-PBS_right4</th>\n",
       "      <th>AfterRTT_left4</th>\n",
       "      <th>PBSlen</th>\n",
       "      <th>RTlen</th>\n",
       "      <th>RT-PBSlen</th>\n",
       "      <th>Edit_pos</th>\n",
       "      <th>Edit_len</th>\n",
       "      <th>...</th>\n",
       "      <th>fGCcont2</th>\n",
       "      <th>fGCcont3</th>\n",
       "      <th>MFE1</th>\n",
       "      <th>MFE2</th>\n",
       "      <th>MFE3</th>\n",
       "      <th>MFE4</th>\n",
       "      <th>MFE5</th>\n",
       "      <th>DeepSpCas9_score</th>\n",
       "      <th>Measured_PE_efficiency</th>\n",
       "      <th>Fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...</td>\n",
       "      <td>xxxxxxxxxxGTGGAGGGAGGTCCGGGGTGAAAATxxxxxxxxxxx...</td>\n",
       "      <td>AAAT</td>\n",
       "      <td>TCAT</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>-50.4</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-25.4</td>\n",
       "      <td>21.012074</td>\n",
       "      <td>9.890110</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...</td>\n",
       "      <td>xxxxxxxxGCGTGGAGGGAGGTCCGGGGTGAAxxxxxxxxxxxxxx...</td>\n",
       "      <td>TGAA</td>\n",
       "      <td>AAAT</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>70.833336</td>\n",
       "      <td>-57.5</td>\n",
       "      <td>-25.7</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-25.4</td>\n",
       "      <td>21.012074</td>\n",
       "      <td>3.436989</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...</td>\n",
       "      <td>xxxxxxxxxxxxxxxxxxAGGTCCGGGGTxxxxxxxxxxxxxxxxx...</td>\n",
       "      <td>GGGT</td>\n",
       "      <td>TGAA</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>72.727270</td>\n",
       "      <td>-31.5</td>\n",
       "      <td>-22.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-25.4</td>\n",
       "      <td>21.012074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...</td>\n",
       "      <td>xxxxxxxxxxxTGGAGGGAGGTCCGGGGxxxxxxxxxxxxxxxxxx...</td>\n",
       "      <td>GGGG</td>\n",
       "      <td>GTGA</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>85.714290</td>\n",
       "      <td>76.470590</td>\n",
       "      <td>-50.2</td>\n",
       "      <td>-27.7</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-25.4</td>\n",
       "      <td>21.012074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...</td>\n",
       "      <td>xxxxxTTGGCGTGGAGGGAGGTCCGGGGTGAAAATCATATCTATTC...</td>\n",
       "      <td>ATTC</td>\n",
       "      <td>CTGA</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>51.219513</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-25.7</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-25.4</td>\n",
       "      <td>21.012074</td>\n",
       "      <td>1.345292</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288788</th>\n",
       "      <td>548131</td>\n",
       "      <td>TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...</td>\n",
       "      <td>xxxxxxxxxxxxxxxxxxTCCACTTGGATGCGGGGGACTTTCTAAG...</td>\n",
       "      <td>CAAT</td>\n",
       "      <td>AACT</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666668</td>\n",
       "      <td>48.484848</td>\n",
       "      <td>-26.1</td>\n",
       "      <td>-24.6</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>16.009068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288789</th>\n",
       "      <td>548132</td>\n",
       "      <td>TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...</td>\n",
       "      <td>xxxxxxxxxxxxxxxxCATCCACTTGGATGCGGGGGACTTTCTAAG...</td>\n",
       "      <td>AACT</td>\n",
       "      <td>CGCA</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>44.117645</td>\n",
       "      <td>46.153847</td>\n",
       "      <td>-28.7</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-7.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>16.009068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288790</th>\n",
       "      <td>548134</td>\n",
       "      <td>TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...</td>\n",
       "      <td>xxxxxxxxxxxxxxTTCATCCACTTGGATGCGGGGGACTTTCTAAG...</td>\n",
       "      <td>ACAA</td>\n",
       "      <td>TAAC</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>48.275864</td>\n",
       "      <td>47.222220</td>\n",
       "      <td>-32.3</td>\n",
       "      <td>-25.7</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>16.009068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288791</th>\n",
       "      <td>548135</td>\n",
       "      <td>TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...</td>\n",
       "      <td>xxxxxxxxxxxxxxxxxxxCCACTTGGATGCGGGGGACTTTCTxxx...</td>\n",
       "      <td>TTCT</td>\n",
       "      <td>AAGA</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>54.545456</td>\n",
       "      <td>58.333332</td>\n",
       "      <td>-24.4</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>16.009068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288792</th>\n",
       "      <td>548136</td>\n",
       "      <td>TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...</td>\n",
       "      <td>xxxxxxxxxxxxxxxxCATCCACTTGGATGCGGGGGACTTTCTAAG...</td>\n",
       "      <td>AGTG</td>\n",
       "      <td>AGCA</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>48.780487</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-39.2</td>\n",
       "      <td>-36.2</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>16.009068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288793 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           No.                                            WT74_On  \\\n",
       "0            1  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
       "1            2  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
       "2            3  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
       "3            4  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
       "4            5  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
       "...        ...                                                ...   \n",
       "288788  548131  TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...   \n",
       "288789  548132  TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...   \n",
       "288790  548134  TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...   \n",
       "288791  548135  TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...   \n",
       "288792  548136  TTTATTTCTGTAATTTCATCCACTTGGATGCGGGGTTGTTTCTAAG...   \n",
       "\n",
       "                                              Edited74_On RT-PBS_right4  \\\n",
       "0       xxxxxxxxxxGTGGAGGGAGGTCCGGGGTGAAAATxxxxxxxxxxx...          AAAT   \n",
       "1       xxxxxxxxGCGTGGAGGGAGGTCCGGGGTGAAxxxxxxxxxxxxxx...          TGAA   \n",
       "2       xxxxxxxxxxxxxxxxxxAGGTCCGGGGTxxxxxxxxxxxxxxxxx...          GGGT   \n",
       "3       xxxxxxxxxxxTGGAGGGAGGTCCGGGGxxxxxxxxxxxxxxxxxx...          GGGG   \n",
       "4       xxxxxTTGGCGTGGAGGGAGGTCCGGGGTGAAAATCATATCTATTC...          ATTC   \n",
       "...                                                   ...           ...   \n",
       "288788  xxxxxxxxxxxxxxxxxxTCCACTTGGATGCGGGGGACTTTCTAAG...          CAAT   \n",
       "288789  xxxxxxxxxxxxxxxxCATCCACTTGGATGCGGGGGACTTTCTAAG...          AACT   \n",
       "288790  xxxxxxxxxxxxxxTTCATCCACTTGGATGCGGGGGACTTTCTAAG...          ACAA   \n",
       "288791  xxxxxxxxxxxxxxxxxxxCCACTTGGATGCGGGGGACTTTCTxxx...          TTCT   \n",
       "288792  xxxxxxxxxxxxxxxxCATCCACTTGGATGCGGGGGACTTTCTAAG...          AGTG   \n",
       "\n",
       "       AfterRTT_left4  PBSlen  RTlen  RT-PBSlen  Edit_pos  Edit_len  ...  \\\n",
       "0                TCAT      11     14         25         4         1  ...   \n",
       "1                AAAT      13     11         24         4         1  ...   \n",
       "2                TGAA       3      8         11         4         1  ...   \n",
       "3                GTGA      10      7         17         4         1  ...   \n",
       "4                CTGA      16     25         41         4         1  ...   \n",
       "...               ...     ...    ...        ...       ...       ...  ...   \n",
       "288788           AACT       3     30         33        15         3  ...   \n",
       "288789           CGCA       5     34         39        15         3  ...   \n",
       "288790           TAAC       7     29         36        15         3  ...   \n",
       "288791           AAGA       2     22         24        15         3  ...   \n",
       "288792           AGCA       5     41         46        15         3  ...   \n",
       "\n",
       "         fGCcont2   fGCcont3  MFE1  MFE2  MFE3  MFE4  MFE5  DeepSpCas9_score  \\\n",
       "0       50.000000  60.000000 -50.4 -25.0  -2.0  -2.3 -25.4         21.012074   \n",
       "1       63.636364  70.833336 -57.5 -25.7  -2.0  -2.3 -25.4         21.012074   \n",
       "2       75.000000  72.727270 -31.5 -22.4   0.0  -2.3 -25.4         21.012074   \n",
       "3       85.714290  76.470590 -50.2 -27.7  -2.0  -2.3 -25.4         21.012074   \n",
       "4       40.000000  51.219513 -62.0 -25.7  -2.0  -2.3 -25.4         21.012074   \n",
       "...           ...        ...   ...   ...   ...   ...   ...               ...   \n",
       "288788  46.666668  48.484848 -26.1 -24.6  -6.0   0.0 -18.7         16.009068   \n",
       "288789  44.117645  46.153847 -28.7 -26.0  -7.4   0.0 -18.7         16.009068   \n",
       "288790  48.275864  47.222220 -32.3 -25.7  -6.4   0.0 -18.7         16.009068   \n",
       "288791  54.545456  58.333332 -24.4 -22.0  -4.3   0.0 -18.7         16.009068   \n",
       "288792  48.780487  50.000000 -39.2 -36.2  -8.2   0.0 -18.7         16.009068   \n",
       "\n",
       "        Measured_PE_efficiency  Fold  \n",
       "0                     9.890110  Test  \n",
       "1                     3.436989  Test  \n",
       "2                     0.000000  Test  \n",
       "3                     0.000000  Test  \n",
       "4                     1.345292  Test  \n",
       "...                        ...   ...  \n",
       "288788                0.000000     4  \n",
       "288789                0.000000     4  \n",
       "288790                0.000000     4  \n",
       "288791                0.000000     4  \n",
       "288792                0.000000     4  \n",
       "\n",
       "[288793 rows x 34 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test' '2' '4' '0' '1' '3']\n"
     ]
    }
   ],
   "source": [
    "print(df['Fold'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold\n",
      "0       66797\n",
      "1       59465\n",
      "2       54496\n",
      "3       48065\n",
      "4       31087\n",
      "Test    28883\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Fold'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9818962"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chidr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['[A]', '[T]', '[G]', '[C]', '[N]', '[PAD]']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DT.csv\")\n",
    "\n",
    "# Convert all numerical columns to float32\n",
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9818962"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRISPRDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequences\n",
    "        wt_seq = self.df['WT74_On'][idx].lower()\n",
    "        edited_seq = self.df['Edited74_On'][idx].lower()\n",
    "\n",
    "        # Replace 'x' with '[PAD]' for better tokenization\n",
    "        edited_seq = edited_seq.replace('x', '[PAD]')\n",
    "\n",
    "        # Create feature dictionary\n",
    "        features = {\n",
    "            'PBS_len': self.df['PBSlen'][idx],\n",
    "            'RT_len': self.df['RTlen'][idx],\n",
    "            'edit_pos': self.df['Edit_pos'][idx],\n",
    "            'edit_len': self.df['Edit_len'][idx],\n",
    "            'RHA_len': self.df['RHA_len'][idx],\n",
    "            'type_sub': self.df['type_sub'][idx],\n",
    "            'type_ins': self.df['type_ins'][idx],\n",
    "            'type_del': self.df['type_del'][idx],\n",
    "            'GC_content': self.calculate_gc_content(wt_seq),\n",
    "            'tm_features': torch.tensor([\n",
    "                self.df['Tm1'][idx],\n",
    "                self.df['Tm2'][idx],\n",
    "                self.df['Tm3'][idx],\n",
    "                self.df['Tm4'][idx],\n",
    "            ], dtype=torch.float32),\n",
    "            'mfe_features': torch.tensor([\n",
    "                self.df['MFE1'][idx],\n",
    "                self.df['MFE2'][idx],\n",
    "                self.df['MFE3'][idx],\n",
    "                self.df['MFE4'][idx],\n",
    "                self.df['MFE5'][idx],\n",
    "            ], dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "        # Tokenize sequences\n",
    "        wt_tokens = self.tokenize_sequence(wt_seq)\n",
    "        edited_tokens = self.tokenize_sequence(edited_seq)\n",
    "\n",
    "        # Combine all features\n",
    "        inputs = {\n",
    "            'wt_tokens': wt_tokens,\n",
    "            'edited_tokens': edited_tokens,\n",
    "            'features': features,\n",
    "        }\n",
    "\n",
    "        # Target\n",
    "        target = torch.tensor(self.df['Measured_PE_efficiency'][idx], dtype=torch.float32)\n",
    "\n",
    "        return inputs, target\n",
    "\n",
    "    def tokenize_sequence(self, sequence):\n",
    "        \"\"\"Tokenize DNA sequence using BERT tokenizer\"\"\"\n",
    "        # Split sequence into individual nucleotides\n",
    "        seq_chars = \" \".join(list(sequence))\n",
    "        tokens = self.tokenizer(\n",
    "            seq_chars,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in tokens.items()}\n",
    "\n",
    "    def calculate_gc_content(self, sequence):\n",
    "        \"\"\"Calculate GC content of sequence\"\"\"\n",
    "        seq = sequence.lower().replace('x', '')\n",
    "        if not seq:\n",
    "            return 0\n",
    "        gc_count = seq.count('g') + seq.count('c')\n",
    "        return gc_count / len(seq)\n",
    "\n",
    "def prepare_crispr_data(df_path, batch_size=32):\n",
    "    \"\"\"Prepare CRISPR data for training\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    # Split data\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create tokenizer for DNA sequences\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # Add special tokens for nucleotides\n",
    "    special_tokens = {'additional_special_tokens': ['[A]', '[T]', '[G]', '[C]', '[N]', '[PAD]']}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CRISPRDataset(train_df, tokenizer)\n",
    "    val_dataset = CRISPRDataset(val_df, tokenizer)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, tokenizer\n",
    "df_path = \"DT.csv\"\n",
    "train_loader, val_loader, tokenizer = prepare_crispr_data(df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No.                                            WT74_On  \\\n",
      "0    1  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
      "1    2  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
      "2    3  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
      "3    4  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
      "4    5  GTGTTTTGGCGTGGAGGGAGGTCCAGGGGTGAAAATCATATCTATT...   \n",
      "\n",
      "                                         Edited74_On RT-PBS_right4  \\\n",
      "0  xxxxxxxxxxGTGGAGGGAGGTCCGGGGTGAAAATxxxxxxxxxxx...          AAAT   \n",
      "1  xxxxxxxxGCGTGGAGGGAGGTCCGGGGTGAAxxxxxxxxxxxxxx...          TGAA   \n",
      "2  xxxxxxxxxxxxxxxxxxAGGTCCGGGGTxxxxxxxxxxxxxxxxx...          GGGT   \n",
      "3  xxxxxxxxxxxTGGAGGGAGGTCCGGGGxxxxxxxxxxxxxxxxxx...          GGGG   \n",
      "4  xxxxxTTGGCGTGGAGGGAGGTCCGGGGTGAAAATCATATCTATTC...          ATTC   \n",
      "\n",
      "  AfterRTT_left4  PBSlen  RTlen  RT-PBSlen  Edit_pos  Edit_len  ...  \\\n",
      "0           TCAT      11     14         25         4         1  ...   \n",
      "1           AAAT      13     11         24         4         1  ...   \n",
      "2           TGAA       3      8         11         4         1  ...   \n",
      "3           GTGA      10      7         17         4         1  ...   \n",
      "4           CTGA      16     25         41         4         1  ...   \n",
      "\n",
      "    fGCcont2   fGCcont3       MFE1       MFE2  MFE3  MFE4  MFE5  \\\n",
      "0  50.000000  60.000000 -50.400002 -25.000000  -2.0  -2.3 -25.4   \n",
      "1  63.636364  70.833336 -57.500000 -25.700001  -2.0  -2.3 -25.4   \n",
      "2  75.000000  72.727272 -31.500000 -22.400000   0.0  -2.3 -25.4   \n",
      "3  85.714287  76.470589 -50.200001 -27.700001  -2.0  -2.3 -25.4   \n",
      "4  40.000000  51.219513 -62.000000 -25.700001  -2.0  -2.3 -25.4   \n",
      "\n",
      "   DeepSpCas9_score  Measured_PE_efficiency  Fold  \n",
      "0         21.012074                9.890110  Test  \n",
      "1         21.012074                3.436989  Test  \n",
      "2         21.012074                0.000000  Test  \n",
      "3         21.012074                0.000000  Test  \n",
      "4         21.012074                1.345291  Test  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "class IntegratedCRISPRDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer=None, use_bert=True, use_onehot=True, max_length=128, max_edited_length=10):\n",
    "        \"\"\"\n",
    "        Integrated dataset that combines one-hot encoding and BERT tokenization.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing CRISPR data.\n",
    "            tokenizer: BERT tokenizer (if use_bert=True).\n",
    "            use_bert: Whether to use BERT tokenization.\n",
    "            use_onehot: Whether to use one-hot encoding.\n",
    "            max_length: Maximum sequence length for tokenization.\n",
    "            max_edited_length: Maximum length of edits.\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_bert = use_bert\n",
    "        self.use_onehot = use_onehot\n",
    "        self.max_length = max_length\n",
    "        self.max_edited_length = max_edited_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if not (use_bert or use_onehot):\n",
    "            raise ValueError(\"At least one of use_bert or use_onehot must be True\")\n",
    "\n",
    "        # Precompute max sequence length\n",
    "        self.total_max_length = max(len(seq) for seq in df['WT74_On']) + max_edited_length\n",
    "\n",
    "        # Preprocess one-hot features to improve efficiency if enabled\n",
    "        if self.use_onehot:\n",
    "            print(\"Preprocessing one-hot encodings...\")\n",
    "            self.onehot_features = self._preprocess_onehot_features()\n",
    "\n",
    "    def _preprocess_onehot_features(self):\n",
    "        \"\"\"Preprocess all sequences with one-hot encoding method.\"\"\"\n",
    "        onehot_features = []\n",
    "\n",
    "        for i in tqdm(range(len(self.df))):\n",
    "            # Get sequences and convert to lower case\n",
    "            dna = self.df['WT74_On'][i].lower()\n",
    "            target = self.df['Edited74_On'][i].lower()\n",
    "\n",
    "            # Determine PBS start position based on first non-'x' in target\n",
    "            for ii in range(len(dna)):\n",
    "                if ii < len(target) and target[ii] != 'x':\n",
    "                    break\n",
    "\n",
    "            pbs_start = ii\n",
    "            pbs_end = ii + self.df['PBSlen'][i]\n",
    "            rtt_start = pbs_end + 1\n",
    "            edit_start = pbs_end + self.df['Edit_pos'][i]\n",
    "            edit_length = self.df['Edit_len'][i]\n",
    "\n",
    "            # Process edit types\n",
    "            if self.df['type_del'][i] == 1:\n",
    "                edit_end = edit_start + edit_length\n",
    "                target = target[:edit_start-1] + 'd' * edit_length + target[edit_start-1:]\n",
    "            elif self.df['type_ins'][i] == 1:\n",
    "                edit_end = edit_start + edit_length\n",
    "                dna = dna[:edit_start] + 'd' * edit_length + dna[edit_start:]\n",
    "            elif self.df['type_sub'][i] == 1:\n",
    "                edit_end = edit_start + edit_length\n",
    "\n",
    "            # Find RHA end (last index with non-'x' in target)\n",
    "            for iii in reversed(range(min(len(dna), len(target)))):\n",
    "                if iii < len(target) and target[iii] != 'x':\n",
    "                    break\n",
    "            rha_end = iii\n",
    "\n",
    "            # Add padding to sequences\n",
    "            padding_dna = self.total_max_length - len(dna)\n",
    "            dna_padded = dna + 'x' * padding_dna\n",
    "\n",
    "            padding_target = self.total_max_length - len(target)\n",
    "            target_padded = target + 'x' * padding_target\n",
    "\n",
    "            # Convert sequences to one-hot encoding\n",
    "            dna_onehot = self._letter_to_onehot(dna_padded)\n",
    "            target_onehot = self._letter_to_onehot(target_padded)\n",
    "\n",
    "            # Create feature masks for PBS, deletion/insertion/substitution, and RTT regions\n",
    "            pbs = torch.zeros(len(target_padded))\n",
    "            pbs[pbs_start:pbs_end] = 1\n",
    "            pbs = pbs.long().unsqueeze(dim=1)\n",
    "\n",
    "            del_target = torch.zeros(len(target_padded))\n",
    "            ins_dna = torch.zeros(len(dna_padded))\n",
    "            sub_dna = torch.zeros(len(dna_padded))\n",
    "\n",
    "            if self.df['type_del'][i] == 1:\n",
    "                del_target[edit_start-1:edit_end-1] = 1\n",
    "                rtt = torch.zeros(len(target_padded))\n",
    "                rtt[rtt_start-1:rha_end+1] = 1\n",
    "                rtt[edit_start-1:edit_end-1] = 0\n",
    "                rtt = rtt.long().unsqueeze(dim=1)\n",
    "            elif self.df['type_ins'][i] == 1:\n",
    "                ins_dna[edit_start:edit_end] = 1\n",
    "                rtt = torch.zeros(len(target_padded))\n",
    "                rtt[rtt_start-1:rha_end+1] = 1\n",
    "                rtt[edit_start:edit_end] = 0\n",
    "                rtt = rtt.long().unsqueeze(dim=1)\n",
    "            elif self.df['type_sub'][i] == 1:\n",
    "                sub_dna[edit_start-1:edit_end-1] = 1\n",
    "                rtt = torch.zeros(len(target_padded))\n",
    "                rtt[rtt_start-1:rha_end+1] = 1\n",
    "                rtt[edit_start-1:edit_end-1] = 0\n",
    "                rtt = rtt.long().unsqueeze(dim=1)\n",
    "\n",
    "            del_target = del_target.long().unsqueeze(dim=1)\n",
    "            ins_dna = ins_dna.long().unsqueeze(dim=1)\n",
    "            sub_dna = sub_dna.long().unsqueeze(dim=1)\n",
    "\n",
    "            # Combine all one-hot features into a single tensor per sequence\n",
    "            result = torch.cat([dna_onehot, target_onehot, pbs, rtt, del_target, ins_dna, sub_dna], dim=1)\n",
    "\n",
    "            onehot_features.append(result)\n",
    "\n",
    "        return onehot_features\n",
    "\n",
    "    def _letter_to_onehot(self, dna):\n",
    "        \"\"\"Convert DNA sequence to one-hot encoding.\"\"\"\n",
    "        dna = dna.lower()\n",
    "        mapping = {'a': 0, 'c': 1, 't': 2, 'g': 3, 'x': 4, 'd': 4}\n",
    "        encoded = torch.zeros((len(dna), 5), dtype=torch.long)\n",
    "        for i, nucleotide in enumerate(dna):\n",
    "            encoded[i, mapping.get(nucleotide, 4)] = 1\n",
    "        return encoded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def calculate_gc_content(self, sequence):\n",
    "        \"\"\"Calculate GC content of sequence.\"\"\"\n",
    "        seq = sequence.lower().replace('x', '')\n",
    "        if not seq:\n",
    "            return 0\n",
    "        gc_count = seq.count('g') + seq.count('c')\n",
    "        return gc_count / len(seq)\n",
    "\n",
    "    def tokenize_sequence(self, sequence):\n",
    "        \"\"\"Tokenize DNA sequence using BERT tokenizer.\"\"\"\n",
    "        seq_chars = \" \".join(list(sequence))  # Space-separate nucleotides\n",
    "        tokens = self.tokenizer(\n",
    "            seq_chars,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in tokens.items()}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve raw sequences\n",
    "        wt_seq = self.df.loc[idx, 'WT74_On'].lower()\n",
    "        edited_seq = self.df.loc[idx, 'Edited74_On'].lower()\n",
    "\n",
    "        # Initialize features dictionary\n",
    "        features = {}\n",
    "\n",
    "        # Add one-hot encoded features if enabled\n",
    "        if self.use_onehot:\n",
    "            features['onehot_encoding'] = self.onehot_features[idx]\n",
    "\n",
    "        # Add BERT tokenized features if enabled\n",
    "        if self.use_bert:\n",
    "            # Clean the edited sequence for tokenization\n",
    "            edited_seq_cleaned = edited_seq.replace('x', '[PAD]')\n",
    "            features['wt_tokens'] = self.tokenize_sequence(wt_seq)\n",
    "            features['edited_tokens'] = self.tokenize_sequence(edited_seq_cleaned)\n",
    "\n",
    "        # Add numerical features\n",
    "        numerical_features = {\n",
    "            'PBS_len': self.df.loc[idx, 'PBSlen'],\n",
    "            'RT_len': self.df.loc[idx, 'RTlen'],\n",
    "            'edit_pos': self.df.loc[idx, 'Edit_pos'],\n",
    "            'edit_len': self.df.loc[idx, 'Edit_len'],\n",
    "            'RHA_len': self.df.loc[idx, 'RHA_len'],\n",
    "            'type_sub': self.df.loc[idx, 'type_sub'],\n",
    "            'type_ins': self.df.loc[idx, 'type_ins'],\n",
    "            'type_del': self.df.loc[idx, 'type_del'],\n",
    "            'GC_content': self.calculate_gc_content(wt_seq),\n",
    "        }\n",
    "\n",
    "        if 'Tm1' in self.df.columns:\n",
    "            numerical_features['tm_features'] = torch.tensor([\n",
    "                self.df.loc[idx, 'Tm1'],\n",
    "                self.df.loc[idx, 'Tm2'],\n",
    "                self.df.loc[idx, 'Tm3'],\n",
    "                self.df.loc[idx, 'Tm4'],\n",
    "            ], dtype=torch.float32)\n",
    "\n",
    "        if 'MFE1' in self.df.columns:\n",
    "            numerical_features['mfe_features'] = torch.tensor([\n",
    "                self.df.loc[idx, 'MFE1'],\n",
    "                self.df.loc[idx, 'MFE2'],\n",
    "                self.df.loc[idx, 'MFE3'],\n",
    "                self.df.loc[idx, 'MFE4'],\n",
    "                self.df.loc[idx, 'MFE5'],\n",
    "            ], dtype=torch.float32)\n",
    "\n",
    "        features['numerical_features'] = numerical_features\n",
    "\n",
    "        # Target variable\n",
    "        target = torch.tensor(self.df.loc[idx, 'Measured_PE_efficiency'], dtype=torch.float32)\n",
    "\n",
    "        return features, target\n",
    "\n",
    "def prepare_integrated_crispr_data(df_path, batch_size=32, use_bert=True, use_onehot=True):\n",
    "    \"\"\"Prepare integrated CRISPR data for training.\"\"\"\n",
    "    # Load data based on file extension\n",
    "    if isinstance(df_path, str):\n",
    "        if df_path.endswith('.csv'):\n",
    "            df = pd.read_csv(df_path)\n",
    "        elif df_path.endswith('.xlsx') or df_path.endswith('.xls'):\n",
    "            df = pd.read_excel(df_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {df_path}\")\n",
    "    else:\n",
    "        df = df_path  # Assume it's already a DataFrame\n",
    "\n",
    "    # Split data into train and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    tokenizer = None\n",
    "    if use_bert:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        special_tokens = {'additional_special_tokens': ['[A]', '[T]', '[G]', '[C]', '[N]', '[PAD]']}\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    train_dataset = IntegratedCRISPRDataset(train_df, tokenizer, use_bert, use_onehot)\n",
    "    val_dataset = IntegratedCRISPRDataset(val_df, tokenizer, use_bert, use_onehot)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "\n",
    "class CRISPRPredictor(nn.Module):\n",
    "    def __init__(self, tokenizer, dropout_rate=0.2):\n",
    "        super(CRISPRPredictor, self).__init__()\n",
    "\n",
    "        # DNA sequence encoder using BERT\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # Resize token embeddings to account for additional tokens\n",
    "        self.bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # Hidden dimension from BERT\n",
    "        self.seq_dim = self.bert.config.hidden_size\n",
    "        # Dimension for combined non-sequence features after processing\n",
    "        self.feature_dim = 16\n",
    "\n",
    "        # Embeddings for categorical features\n",
    "        self.pbs_embedding = nn.Embedding(50, 8)         # PBS length embedding\n",
    "        self.rt_embedding = nn.Embedding(50, 8)          # RT length embedding\n",
    "        self.edit_pos_embedding = nn.Embedding(50, 8)\n",
    "        self.edit_len_embedding = nn.Embedding(20, 8)\n",
    "        self.rha_len_embedding = nn.Embedding(50, 8)\n",
    "\n",
    "        # Embedding for edit type (0: sub, 1: ins, 2: del)\n",
    "        self.edit_type_embedding = nn.Embedding(3, 8)\n",
    "\n",
    "        # Linear layers for numerical features\n",
    "        self.tm_features_linear = nn.Linear(4, 16)\n",
    "        self.mfe_features_linear = nn.Linear(5, 16)\n",
    "        self.gc_linear = nn.Linear(1, 8)\n",
    "\n",
    "        # Combine all non-sequence features into a fixed dimension\n",
    "        # Total concatenated dimension: 8*5 (from 5 embeddings) + 8 (edit type) + 16 (tm) + 16 (mfe) + 8 (gc) = 80\n",
    "        self.feature_linear = nn.Linear(80, self.feature_dim)\n",
    "\n",
    "        # Self-attention mechanism for sequence features\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.seq_dim, num_heads=8, dropout=dropout_rate)\n",
    "\n",
    "        # Final combined processing\n",
    "        self.combined_linear = nn.Sequential(\n",
    "            nn.Linear(self.seq_dim + self.feature_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        # Sigmoid activation to scale output (we later multiply by 100 for percentage)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Process sequence data with BERT:\n",
    "        wt_output = self.bert(**inputs['wt_tokens']).last_hidden_state  # shape: [batch, seq_len, hidden_size]\n",
    "        edited_output = self.bert(**inputs['edited_tokens']).last_hidden_state\n",
    "\n",
    "        # Apply self-attention to capture context (using the same tensor for query, key, value)\n",
    "        wt_attn, _ = self.attention(wt_output, wt_output, wt_output)\n",
    "        edited_attn, _ = self.attention(edited_output, edited_output, edited_output)\n",
    "\n",
    "        # Mean pooling over sequence length\n",
    "        wt_pooled = torch.mean(wt_attn, dim=1)       # shape: [batch, hidden_size]\n",
    "        edited_pooled = torch.mean(edited_attn, dim=1)\n",
    "\n",
    "        # Difference between edited and wild-type to capture change\n",
    "        seq_features = edited_pooled - wt_pooled      # shape: [batch, hidden_size]\n",
    "\n",
    "        # Process numerical/categorical features.\n",
    "        # NOTE: Our IntegratedCRISPRDataset returns these under 'numerical_features'\n",
    "        features = inputs['numerical_features']\n",
    "\n",
    "        # Embed categorical features (clamping values to avoid index errors)\n",
    "        pbs_emb = self.pbs_embedding(torch.clamp(features['PBS_len'], max=49))\n",
    "        rt_emb = self.rt_embedding(torch.clamp(features['RT_len'], max=49))\n",
    "        edit_pos_emb = self.edit_pos_embedding(torch.clamp(features['edit_pos'], max=49))\n",
    "        edit_len_emb = self.edit_len_embedding(torch.clamp(features['edit_len'], max=19))\n",
    "        rha_len_emb = self.rha_len_embedding(torch.clamp(features['RHA_len'], max=49))\n",
    "\n",
    "        # Determine edit type: stack type_sub, type_ins, type_del and take argmax along dimension 1\n",
    "        edit_type = torch.argmax(torch.stack([\n",
    "            features['type_sub'],\n",
    "            features['type_ins'],\n",
    "            features['type_del']\n",
    "        ], dim=1), dim=1)\n",
    "        edit_type_emb = self.edit_type_embedding(edit_type)\n",
    "\n",
    "        # Process continuous numerical features\n",
    "        tm_features = self.tm_features_linear(features['tm_features']).float()\n",
    "        mfe_features = self.mfe_features_linear(features['mfe_features']).float()\n",
    "        gc_features = self.gc_linear(features['GC_content'].unsqueeze(1)).float()\n",
    "\n",
    "        # Concatenate all non-sequence features\n",
    "        combined = torch.cat([\n",
    "            pbs_emb, rt_emb, edit_pos_emb, edit_len_emb, rha_len_emb,\n",
    "            edit_type_emb, tm_features, mfe_features, gc_features\n",
    "        ], dim=1)  # shape: [batch, 80]\n",
    "\n",
    "        # Transform to fixed feature dimension\n",
    "        feature_vector = self.feature_linear(combined)  # shape: [batch, feature_dim]\n",
    "\n",
    "        # Concatenate sequence features with processed non-sequence features\n",
    "        final_features = torch.cat([seq_features, feature_vector.float()], dim=1)  # shape: [batch, seq_dim + feature_dim]\n",
    "\n",
    "        # Pass through final prediction layers\n",
    "        output = self.combined_linear(final_features)\n",
    "\n",
    "        # Scale the output to a percentage (0-100)\n",
    "        return self.sigmoid(output) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRISPRPredictor(tokenizer).to(device).to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Preprocessing one-hot encodings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231034/231034 [03:35<00:00, 1072.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing one-hot encodings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57759/57759 [01:03<00:00, 911.39it/s] \n",
      "C:\\Users\\chidr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 169\u001b[0m\n\u001b[0;32m    161\u001b[0m train_loader, val_loader, tokenizer \u001b[38;5;241m=\u001b[39m prepare_integrated_crispr_data(\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDT.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    163\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m    164\u001b[0m     use_bert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    165\u001b[0m     use_onehot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    166\u001b[0m )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m model, best_loss, r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_crispr_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed with best validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and RÂ² score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 57\u001b[0m, in \u001b[0;36mtrain_crispr_model\u001b[1;34m(train_loader, val_loader, tokenizer, device, epochs)\u001b[0m\n\u001b[0;32m     54\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 57\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), targets)\n\u001b[0;32m     59\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[56], line 82\u001b[0m, in \u001b[0;36mCRISPRPredictor.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     79\u001b[0m features \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumerical_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Embed categorical features (clamping values to avoid index errors)\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m pbs_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpbs_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPBS_len\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m49\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m rt_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrt_embedding(torch\u001b[38;5;241m.\u001b[39mclamp(features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRT_len\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m49\u001b[39m))\n\u001b[0;32m     84\u001b[0m edit_pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medit_pos_embedding(torch\u001b[38;5;241m.\u001b[39mclamp(features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medit_pos\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m49\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def train_crispr_model(train_loader, val_loader, tokenizer, device='cuda', epochs=50):\n",
    "    # Initialize the model and move it to the specified device\n",
    "    model = CRISPRPredictor(tokenizer).to(device)\n",
    "\n",
    "    # Set up optimizer with weight decay for regularization\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    # Set up a learning rate scheduler that reduces LR when the validation loss plateaus\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    # Mean Squared Error loss for regression\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_r2_scores = []\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # Move BERT tokenized inputs to device\n",
    "            wt_tokens = {k: v.to(device).long() for k, v in inputs['wt_tokens'].items()}\n",
    "            edited_tokens = {k: v.to(device).long() for k, v in inputs['edited_tokens'].items()}\n",
    "\n",
    "            # Move numerical features to device (our integrated dataset returns these under 'numerical_features')\n",
    "            features = {}\n",
    "            for k, v in inputs['numerical_features'].items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    features[k] = v.to(device).float()\n",
    "                else:\n",
    "                    features[k] = torch.tensor(v, device=device, dtype=torch.float32)\n",
    "\n",
    "            batch_inputs = {\n",
    "                'wt_tokens': wt_tokens,\n",
    "                'edited_tokens': edited_tokens,\n",
    "                'numerical_features': features\n",
    "            }\n",
    "            targets = targets.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs).float()\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                wt_tokens = {k: v.to(device) for k, v in inputs['wt_tokens'].items()}\n",
    "                edited_tokens = {k: v.to(device) for k, v in inputs['edited_tokens'].items()}\n",
    "                features = {}\n",
    "                for k, v in inputs['numerical_features'].items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        features[k] = v.to(device).float()\n",
    "                    else:\n",
    "                        features[k] = torch.tensor(v, device=device, dtype=torch.float32)\n",
    "\n",
    "                batch_inputs = {\n",
    "                    'wt_tokens': wt_tokens,\n",
    "                    'edited_tokens': edited_tokens,\n",
    "                    'numerical_features': features\n",
    "                }\n",
    "                targets = targets.to(device).float()\n",
    "                outputs = model(batch_inputs).float()\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                all_predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        r2 = r2_score(all_targets, all_predictions)\n",
    "        val_r2_scores.append(r2)\n",
    "        rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save the best model so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model_state,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "                'r2_score': r2,\n",
    "            }, 'best_crispr_model.pt')\n",
    "\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed in {time_taken:.2f}s\")\n",
    "        print(f\"Training Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"RÂ² Score: {r2:.4f} | RMSE: {rmse:.4f}\")\n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Load the best model state for final evaluation\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curves')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_r2_scores, label='RÂ² Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RÂ² Score')\n",
    "    plt.legend()\n",
    "    plt.title('RÂ² Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "\n",
    "    return model, best_val_loss, val_r2_scores[-1]\n",
    "\n",
    "# ---------------------------\n",
    "# Main block to run training\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device (GPU if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Prepare data. Make sure to use the integrated data preparation function.\n",
    "    # Replace 'data/DeepPrime_dataset_final_Feat8.csv' with your actual CSV file path.\n",
    "    train_loader, val_loader, tokenizer = prepare_integrated_crispr_data(\n",
    "        'DT.csv',\n",
    "        batch_size=32,\n",
    "        use_bert=True,\n",
    "        use_onehot=True\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model, best_loss, r2 = train_crispr_model(train_loader, val_loader, tokenizer, device=device, epochs=50)\n",
    "\n",
    "    print(f\"Training completed with best validation loss: {best_loss:.4f} and RÂ² score: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
